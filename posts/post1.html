<!DOCTYPE html>
<div class="container" id="post">
  <h3>Post 1: 1/13/2018</h3>
  <p>This weeks blog post will be focused on discussing the general functionality of Darklight, the software being developed here at Champion Technology Company Inc. (CTCI), how it uses machine learning to better the cyber environment, and what it is currently lacking in functionality that will lead to a later blog post explaining the current project that I am working on as a starting point to filling these gaps.</p>
  <p>Darklight uses the concept of playbook chains to sequence the thinking process that a cyber analyst uses to make decisions in a cyber environment. These playbooks are constructed from a sequence of steps that chain together, passing on information that has been gained or inferred during the chains processing of events that are ingested into the system. As data logs are read into Darklight, they are ingested at the root of a playbook that listens for these events and the processing begins. The steps used in the decision making of a cyber analyst range from “What was the time of this event?” or “What machine was this event from?” are the same ones that are replicated in the step library: the list of all the steps usable in Darklight.</p>
  <p>The steps are the core feature of Darklight as they are the building blocks of constructing decision making processes via playbooks. There is one responsibility/output for each step. The reasoning behind this is to simply the sequence to extract or manipulate one variable of the equation at a time, providing more flexibility and somewhat forcing the analyst to really understand what it is that they are asking</p>
  <p>Learning implies the extension of knowledge and remembering of past information previously exposed to. Machine learning must do this to replicate what it means for a human to learn but on the level of a computer. The biggest advantage of this is the processing power and speed in which a computer can work on information. And not just that but also infer new information or a linking of knowing information so that to better enrich its understanding.</p>
  <p>As we further develop Darklight, we continue to research and implement new features that better place the concept of machine learning within the software as this is the primary goal of the company: to provide software that is better able to understand the context of information by replication of human thinking and decision making.</p>
  <p>Critical understanding of the data going through your enterprise is key to knowing what is considered normal. Each entity in an enterprise has a set of behavior that is part of its definition, whether it be a machine or employee. Knowing behavior means to know patterns and norms. These however are not static but a rather dynamic property. This is where Darklight lacks: it does not have the capability of discovering these patterns, at least in any practical way. With enough determination and patience (and plenty of hacks to go along with them), one could discover these traits, but in many more processes that would be considered practical or reusable across multiple enterprises.</p>
  <p>Over the past two months, I have been researching and implementing a method of being able to calculate statical information over a sample set of data. Statistics is a key method to understanding your data and how new data ‘fits’ with it. The first month was understand the question that was asked by my supervisor that sparked this feature and how it would best fit not only on the front end user side of the software but also how it would fit into the current backend framework. With the assistance and guided thinking of my mentor, Peter Neorr, we have been able to develop a design that would let us know the statistics of data relevant to a playbook.</p>
  <p>The service primarily works with numerical data but will be extended to understand how to manipulate and work with question involving time. At the current stage of development, we have a working implementation of the service that will store data  and allow you to retrieve statistical results. This past Friday was able to give a proof of concept demo to the team at Champion and received a lot of feedback on its use and possible directions to go from here.</p>
  <p>The next challenges involve the management, and essentially garbage collection, of the sample sets of data that are stored in the database. This is more of a learning challenge for me to figure out a method to correlate internal events in the api that I can use to identify when a sample set needs to be removed from the database as to not clutter disk space.</p>
  <p>Optimization is also a large concern in making this service scaleable. In our test environment, it is really hard to replication the degree of dataflow an enterprise such as AT&T or Netflix is capable of producing. Keeping that in mind, I will also have to gain further understanding of JPA (Java Persistence API) and SQL queries to better optimize the extraction of data from the database considering that millions of transactions could happen an hour. Later blog posts will document the challenges I face across the way and how I solve certain solutions.</p> 
</div>